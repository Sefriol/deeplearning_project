{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Torch CUDA torch device\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored in fruits\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "    #transforms.Normalize((0.5,), (0.5,))  # Min-max scaling to [-1, 1]\n",
    "])\n",
    "\n",
    "data_dir = os.path.join('fruits')\n",
    "print('Data stored in %s' % data_dir)\n",
    "\n",
    "trainset = ImageFolder(\"./fruits/Training\",transform=transform)\n",
    "testset = ImageFolder(\"./fruits/Test\",transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels():\n",
    "    trainset_labels = []\n",
    "    testset_labels = []\n",
    "    for i in trainset.imgs:\n",
    "        trainset_labels.append(i[1])\n",
    "    \n",
    "    for j in testset.imgs:\n",
    "        testset_labels.append(j[1])\n",
    "    \n",
    "    return (trainset_labels, testset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset has total of 103 classes\n"
     ]
    }
   ],
   "source": [
    "# Total classes\n",
    "classes_idx_dict = trainset.class_to_idx # {'Class Name': idx }\n",
    "classes = len(trainset.classes)\n",
    "len_trainset = len(trainset)\n",
    "len_testset = len(testset)\n",
    "\n",
    "train_labels, test_labels = generate_labels()\n",
    "print(f'Trainset has total of {classes} classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: batch size=150, channels=3, image height=100, image width=100\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=150, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=150, shuffle=False)\n",
    "\n",
    "image_shape = iter(trainloader).next()[0].shape\n",
    "BATCH_SIZE, CHANNELS, HEIGHT, WIDTH = iter(trainloader).next()[0].shape\n",
    "print(f'Image: batch size={image_shape[0]}, channels={image_shape[1]}, image height={image_shape[2]}, image width={image_shape[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCL(nn.Module):\n",
    "    def __init__(self, K=192):\n",
    "        super(RCL, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(K, K, kernel_size=(3, 3), stride=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(K),\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(K, K, kernel_size=(3, 3), stride=(1,1),padding=(1,1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bnorm = nn.BatchNorm2d(K)\n",
    "\n",
    "    def forward(self, X, verbose=False):\n",
    "        #T = 0\n",
    "        conv1 = self.conv1(X)\n",
    "        \n",
    "        #T = 1\n",
    "        conv2 = self.relu(F.conv2d(input=conv1,weight=self.conv2.weight, padding=(1,1)))\n",
    "        rcl2 = torch.add(conv1, conv2)\n",
    "        bn2 = self.bnorm(rcl2)\n",
    "        \n",
    "        #T = 2\n",
    "        conv3 = self.relu(F.conv2d(input=bn2,weight=self.conv2.weight, padding=(1,1)))\n",
    "        rcl3 = torch.add(conv1, conv3)\n",
    "        bn3 = self.bnorm(rcl3)\n",
    "        \n",
    "        #T = 3\n",
    "        conv4 = self.relu(F.conv2d(input=bn3,weight=self.conv2.weight, padding=(1,1)))\n",
    "        rcl4 = torch.add(conv1, conv4)\n",
    "        bn4 = self.bnorm(rcl4)\n",
    "        \n",
    "        return bn4\n",
    "        \n",
    "class RCNNet(nn.Module):\n",
    "    def __init__(self, cls, K=192):\n",
    "        super(RCNNet, self).__init__()\n",
    "        '''\n",
    "        To save computation, layer 1 is the standard feed-forward convolutional layer\n",
    "        without recurrent connections, followed by max pooling. \n",
    "        Layers 1 to 5 were constrained to have the same number of feature maps K.\n",
    "        Kernel size in layer 1 is 5 × 5, the feed-forward and recurrent filter sizes in layers 2 to 4 were all 3×3.\n",
    "        Dropout is used after each RCL except layer 5, which was connected to the softmax layer\n",
    "        '''\n",
    "        cnn = nn.Sequential()\n",
    "        cnn.add_module(f'Layer 1 (convolution)', nn.Conv2d(3, K, kernel_size=(5,5)))\n",
    "\n",
    "        # Both pooling operations have stride 2 and size 3\n",
    "        cnn.add_module(f'max pooling', nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "\n",
    "        #  On top of this, four RCLs are used \n",
    "        cnn.add_module(f'Layer 2 (recurrent convolution)', RCL(K))\n",
    "        cnn.add_module('Dropout', nn.Dropout(p=0.8))\n",
    "        cnn.add_module(f'Layer 3 (recurrent convolution)', RCL(K))\n",
    "\n",
    "        # with a max pooling layer in the middle\n",
    "        cnn.add_module(f'max pooling', nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "\n",
    "        # If the RCL was followed by a pooling layer, dropout was placed after pooling.\n",
    "        cnn.add_module('Dropout', nn.Dropout(p=0.8))\n",
    "\n",
    "        cnn.add_module(f'Layer 4 (recurrent convolution)', RCL(K))\n",
    "        cnn.add_module('Dropout', nn.Dropout(p=0.8))\n",
    "        cnn.add_module(f'Layer 5 (recurrent convolution)', RCL(K))\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "        self.cnn = cnn\n",
    "        self.out = nn.Linear(K, cls)\n",
    "\n",
    "    def forward(self, input, verbose=False):\n",
    "        output = self.cnn(input)\n",
    "        output = F.max_pool2d(output, kernel_size=(output.shape[1],output.shape[2]),stride=(output.shape[1],output.shape[2]))\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.logsoftmax(output)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCNNet(\n",
      "  (logsoftmax): LogSoftmax()\n",
      "  (cnn): Sequential(\n",
      "    (Layer 1 (convolution)): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (max pooling): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (Layer 2 (recurrent convolution)): RCL(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "      (bnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Dropout): Dropout(p=0.8)\n",
      "    (Layer 3 (recurrent convolution)): RCL(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "      (bnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Layer 4 (recurrent convolution)): RCL(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "      (bnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (Layer 5 (recurrent convolution)): RCL(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "      (bnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=32, out_features=103, bias=True)\n",
      ")\n",
      "Shape of the input tensor: torch.Size([150, 3, 100, 100])\n",
      "32 39\n",
      "torch.Size([150, 103])\n",
      "The shapes seem to be ok.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\dle\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# Let's test the shapes of the tensors\n",
    "net = RCNNet(classes,K=32)\n",
    "net.to(device)\n",
    "print(net)\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "    images = images.to(device)\n",
    "    print('Shape of the input tensor:', images.shape)\n",
    "\n",
    "    y = net(images, verbose=True)\n",
    "    print(y.shape)\n",
    "    assert y.shape == torch.Size([BATCH_SIZE, classes]), f'Bad shape of y: y.shape={y.shape}'\n",
    "\n",
    "print('The shapes seem to be ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "final_learning_rate = 0.00001\n",
    "learning_rate = initial_learning_rate\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\dle\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.727\n",
      "[1,   200] loss: 0.315\n",
      "[1,   300] loss: 0.233\n",
      "Accuracy of the network on the 17845 test images: 60.611%\n",
      "[2,   100] loss: 0.124\n",
      "[2,   200] loss: 0.019\n",
      "[2,   300] loss: 0.027\n",
      "Accuracy of the network on the 17845 test images: 97.405%\n",
      "[3,   100] loss: 0.027\n",
      "[3,   200] loss: 0.010\n",
      "[3,   300] loss: 0.012\n",
      "Accuracy of the network on the 17845 test images: 96.425%\n",
      "[4,   100] loss: 0.055\n",
      "[4,   200] loss: 0.082\n",
      "[4,   300] loss: 0.043\n",
      "Accuracy of the network on the 17845 test images: 96.542%\n",
      "[5,   100] loss: 0.026\n",
      "[5,   200] loss: 0.006\n",
      "[5,   300] loss: 0.003\n",
      "Accuracy of the network on the 17845 test images: 99.159%\n",
      "[6,   100] loss: 0.000\n",
      "[6,   200] loss: 0.000\n",
      "[6,   300] loss: 0.000\n",
      "Accuracy of the network on the 17845 test images: 99.159%\n",
      "[7,   100] loss: 0.000\n",
      "[7,   200] loss: 0.000\n",
      "[7,   300] loss: 0.000\n",
      "Accuracy of the network on the 17845 test images: 99.137%\n",
      "[8,   100] loss: 0.000\n",
      "[8,   200] loss: 0.000\n",
      "[8,   300] loss: 0.000\n",
      "Accuracy of the network on the 17845 test images: 99.115%\n",
      "[9,   100] loss: 0.000\n",
      "[9,   200] loss: 0.000\n",
      "[9,   300] loss: 0.000\n",
      "Accuracy of the network on the 17845 test images: 99.109%\n",
      "[10,   100] loss: 0.000\n",
      "[10,   200] loss: 0.000\n",
      "[10,   300] loss: 0.000\n",
      "Accuracy of the network on the 17845 test images: 99.120%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epochs=10\n",
    "net.train()\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print_every = 100  # mini-batches\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        # Transfer to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i % print_every) == (print_every-1):\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Print accuracy after every epoch\n",
    "    accuracy = compute_accuracy(net, testloader)\n",
    "    print(f'Accuracy of the network on the {len_testset} test images: {100 * accuracy:.3f}%')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\conda\\conda\\envs\\dle\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 0.991\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(net, testloader)\n",
    "print(f'Accuracy of the network on the test images: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to rcnn.pth\n"
     ]
    }
   ],
   "source": [
    "filename = 'rcnn.pth'\n",
    "\n",
    "try:\n",
    "    do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "    if do_save == 'yes':\n",
    "        torch.save(net.state_dict(), filename)\n",
    "        print('Model saved to %s' % filename)\n",
    "    else:\n",
    "        print('Model not saved')\n",
    "except:\n",
    "    raise Exception('The notebook should be run or validated with skip_training=True.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RCNNet()\n",
    "net.load_state_dict(torch.load(filename, map_location=lambda storage, loc: storage))\n",
    "net.to(device)\n",
    "print(f'Model loaded from {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
